{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-2-cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4embtkV0pNxM"
      },
      "source": [
        "Deep Learning with Tensorflow\n",
        "=============\n",
        "\n",
        "Assignment II - Convolutional Neural Networks\n",
        "------------\n",
        "\n",
        "Previously in `20210322-lab-1-notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
        "\n",
        "The goal of this assignment is make the neural network convolutional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY4HnkP-bY--"
      },
      "source": [
        "First reload the data we generated in `20210322-lab-1-notmnist.ipynb`. If you already have the data downloaded, you can just load the saved pickel files. This would work if you are running the notebook locally in jupyter. Then you can just load the data with the following snippet:\n",
        "\n",
        "```\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset = save['train_dataset']\n",
        "  train_labels = save['train_labels']\n",
        "  valid_dataset = save['valid_dataset']\n",
        "  valid_labels = save['valid_labels']\n",
        "  test_dataset = save['test_dataset']\n",
        "  test_labels = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "  print 'Training set', train_dataset.shape, train_labels.shape\n",
        "  print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
        "  print 'Test set', test_dataset.shape, test_labels.shape\n",
        "```\n",
        "\n",
        "If you are running in colab, then just continue with the next two paragraphs. We will redownload the files using the same python methods as in the first lab excercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2cf8zDbcZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65d6931-9f59-4c5f-fbb6-a2ff16d7808e"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/nktaushanov/7aa762a4e1370b5ad287e87595c6499e/raw/4e6ee948d963d4efe16a9452036c6e380d0b30db/download_notmnist.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-18 19:02:40--  https://gist.githubusercontent.com/nktaushanov/7aa762a4e1370b5ad287e87595c6499e/raw/4e6ee948d963d4efe16a9452036c6e380d0b30db/download_notmnist.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7285 (7.1K) [text/plain]\n",
            "Saving to: ‘download_notmnist.py.3’\n",
            "\n",
            "\rdownload_notmnist.p   0%[                    ]       0  --.-KB/s               \rdownload_notmnist.p 100%[===================>]   7.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-18 19:02:40 (55.5 MB/s) - ‘download_notmnist.py.3’ saved [7285/7285]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjfyVXkPbfiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406eb50a-76ad-4717-ef52-8aae687fd690"
      },
      "source": [
        "import download_notmnist\n",
        "train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels = download_notmnist.run()\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found and verified ./notMNIST_large.tar.gz\n",
            "Found and verified ./notMNIST_small.tar.gz\n",
            "./notMNIST_large already present - Skipping extraction of ./notMNIST_large.tar.gz.\n",
            "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
            "./notMNIST_small already present - Skipping extraction of ./notMNIST_small.tar.gz.\n",
            "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n",
            "./notMNIST_large/A.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/B.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/C.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/D.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/E.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/F.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/G.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/H.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/I.pickle already present - Skipping pickling.\n",
            "./notMNIST_large/J.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/A.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/B.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/C.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/D.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/E.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/F.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/G.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/H.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/I.pickle already present - Skipping pickling.\n",
            "./notMNIST_small/J.pickle already present - Skipping pickling.\n",
            "Training: (200000, 28, 28) (200000,)\n",
            "Validation: (10000, 28, 28) (10000,)\n",
            "Testing: (10000, 28, 28) (10000,)\n",
            "Training set (200000, 28, 28) (200000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "tm2CQN_Cpwj0"
      },
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import range"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7aHrm6nGDMB"
      },
      "source": [
        "Reformat into a TensorFlow-friendly shape:\n",
        "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
        "- labels as float 1-hot encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "IRSyYiIIGIzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cc2b18-018d-43d7-ef26-50ea4fdcb2dc"
      },
      "source": [
        "image_size = 28\n",
        "num_labels = 10\n",
        "num_channels = 1 # grayscale\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape(\n",
        "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set (200000, 28, 28, 1) (200000, 10)\n",
            "Validation set (10000, 28, 28, 1) (10000, 10)\n",
            "Test set (10000, 28, 28, 1) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "AgQDIREv02p1"
      },
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwNkLqstiA6E"
      },
      "source": [
        "batch_size = 16\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_dataset, train_labels)).shuffle(1000).batch(batch_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rhgjmROXu2O"
      },
      "source": [
        "## Problem 1\n",
        "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes.\n",
        "\n",
        "Edit the snippet bellow by changing the `model` function.\n",
        "\n",
        "### 1.1 - Define the model\n",
        "Implement the `model` function bellow. Take a look at the following TF functions:\n",
        "- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W1$, this function convolves $W1$'s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
        "- **tf.nn.relu(Z1):** computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "IZYv70SvvOan"
      },
      "source": [
        "batch_size = 16\n",
        "patch_size = 5\n",
        "depth = 16\n",
        "num_hidden = 64\n",
        "\n",
        "layer1_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
        "layer1_biases = tf.Variable(tf.zeros([depth]))\n",
        "layer2_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [patch_size, patch_size, depth, depth], stddev=0.1))\n",
        "layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
        "layer3_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
        "layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
        "layer4_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [num_hidden, num_labels], stddev=0.1))\n",
        "layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
        "\n",
        "def model(data):\n",
        "  conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
        "  hidden = tf.nn.relu(conv + layer1_biases)\n",
        "  conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
        "  hidden = tf.nn.relu(conv + layer2_biases)\n",
        "  shape = hidden.get_shape().as_list()\n",
        "  reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
        "  hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
        "  return tf.matmul(hidden, layer4_weights) + layer4_biases"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBt6URFR7oT4"
      },
      "source": [
        "\n",
        "### 1.2 - Compute loss\n",
        "\n",
        "Implement the `compute_loss` function below. You might find these two functions helpful: \n",
        "\n",
        "- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  [here.](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
        "- **tf.reduce_mean:** computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FJCT1x-7nwF"
      },
      "source": [
        "def compute_loss(labels, logits):\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "  return loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkzpbHET-m8S"
      },
      "source": [
        "### 1.3 - Improve your model\n",
        "Try to achieve a test accuracy of around 80%. Iterate on the filters size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duYGBpFQ6_G8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c9fa3e-9a7d-48ff-a92f-df995da81954"
      },
      "source": [
        "num_steps = 1000\n",
        "display_step = 100\n",
        "learning_rate = 0.05\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Run training for the given number of steps.\n",
        "for step, (batch_x, batch_y) in enumerate(train_tf_dataset.take(num_steps), 1):  \n",
        "  # Training computation.\n",
        "  with tf.GradientTape() as g:\n",
        "    logits = model(batch_x)\n",
        "    loss = compute_loss(batch_y, logits)\n",
        "    \n",
        "  # Optimizer.\n",
        "  optimizer.minimize(loss, g.watched_variables(), tape=g)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  if step % display_step == 0:\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    train_acc = accuracy(train_prediction, batch_y)\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(model(valid_dataset))\n",
        "    valid_acc = accuracy(valid_prediction, valid_labels)\n",
        "    print(\"step: %i, loss: %f, train acc: %f, validation acc: %f\" % (step, loss, train_acc, valid_acc))\n",
        "\n",
        "test_prediction = tf.nn.softmax(model(test_dataset))\n",
        "test_acc = accuracy(test_prediction, test_labels)\n",
        "print(\"test acc: %f\" % (test_acc))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 100, loss: 2.015269, train acc: 43.750000, validation acc: 54.930000\n",
            "step: 200, loss: 0.746942, train acc: 68.750000, validation acc: 77.470000\n",
            "step: 300, loss: 0.995147, train acc: 75.000000, validation acc: 79.000000\n",
            "step: 400, loss: 0.592980, train acc: 81.250000, validation acc: 80.410000\n",
            "step: 500, loss: 0.404649, train acc: 93.750000, validation acc: 81.440000\n",
            "step: 600, loss: 0.814277, train acc: 81.250000, validation acc: 81.810000\n",
            "step: 700, loss: 0.430593, train acc: 87.500000, validation acc: 81.680000\n",
            "step: 800, loss: 0.676272, train acc: 87.500000, validation acc: 82.610000\n",
            "step: 900, loss: 0.469291, train acc: 87.500000, validation acc: 82.720000\n",
            "step: 1000, loss: 0.611028, train acc: 81.250000, validation acc: 83.050000\n",
            "test acc: 89.680000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KedKkn4EutIK"
      },
      "source": [
        "---\n",
        "Problem 2\n",
        "---------\n",
        "\n",
        "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "patch_size = 5\n",
        "depth = 16\n",
        "num_hidden = 64\n",
        "\n",
        "layer1_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
        "layer1_biases = tf.Variable(tf.zeros([depth]))\n",
        "layer2_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [patch_size, patch_size, depth, depth], stddev=0.1))\n",
        "layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
        "layer3_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
        "layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
        "layer4_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [num_hidden, num_labels], stddev=0.1))\n",
        "layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
        "\n",
        "def model_with_max_pool(data):\n",
        "  conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
        "  pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
        "  hidden = tf.nn.relu(pool + layer1_biases)\n",
        "  conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
        "  pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
        "  hidden = tf.nn.relu(pool + layer2_biases)\n",
        "  shape = hidden.get_shape().as_list()\n",
        "  reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
        "  hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
        "  return tf.matmul(hidden, layer4_weights) + layer4_biases"
      ],
      "metadata": {
        "id": "6UmfDoQHY4fl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 1000\n",
        "display_step = 100\n",
        "learning_rate = 0.05\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Run training for the given number of steps.\n",
        "for step, (batch_x, batch_y) in enumerate(train_tf_dataset.take(num_steps), 1):  \n",
        "  # Training computation.\n",
        "  with tf.GradientTape() as g:\n",
        "    logits = model_with_max_pool(batch_x)\n",
        "    loss = compute_loss(batch_y, logits)\n",
        "    \n",
        "  # Optimizer.\n",
        "  optimizer.minimize(loss, g.watched_variables(), tape=g)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  if step % display_step == 0:\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    train_acc = accuracy(train_prediction, batch_y)\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(model(valid_dataset))\n",
        "    valid_acc = accuracy(valid_prediction, valid_labels)\n",
        "    print(\"step: %i, loss: %f, train acc: %f, validation acc: %f\" % (step, loss, train_acc, valid_acc))\n",
        "\n",
        "test_prediction = tf.nn.softmax(model(test_dataset))\n",
        "test_acc = accuracy(test_prediction, test_labels)\n",
        "print(\"test acc: %f\" % (test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPPMCcl1csjC",
        "outputId": "d7f0faf9-7485-4d32-de7b-40ec837c0ddd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 100, loss: 1.355374, train acc: 56.250000, validation acc: 51.470000\n",
            "step: 200, loss: 0.475660, train acc: 87.500000, validation acc: 66.670000\n",
            "step: 300, loss: 0.294201, train acc: 87.500000, validation acc: 64.450000\n",
            "step: 400, loss: 1.022111, train acc: 75.000000, validation acc: 70.030000\n",
            "step: 500, loss: 0.342548, train acc: 87.500000, validation acc: 68.840000\n",
            "step: 600, loss: 0.567398, train acc: 81.250000, validation acc: 73.830000\n",
            "step: 700, loss: 0.328666, train acc: 93.750000, validation acc: 70.500000\n",
            "step: 800, loss: 0.197961, train acc: 100.000000, validation acc: 72.620000\n",
            "step: 900, loss: 0.769935, train acc: 81.250000, validation acc: 73.190000\n",
            "step: 1000, loss: 0.666586, train acc: 75.000000, validation acc: 71.360000\n",
            "test acc: 77.700000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klf21gpbAgb-"
      },
      "source": [
        "---\n",
        "Problem 3\n",
        "---------\n",
        "\n",
        "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "patch_size = 5\n",
        "depth = 16\n",
        "num_hidden = 64\n",
        "\n",
        "layer1_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
        "layer1_biases = tf.Variable(tf.zeros([depth]))\n",
        "layer2_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [patch_size, patch_size, depth, depth], stddev=0.1))\n",
        "layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
        "layer3_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
        "layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
        "layer4_weights = tf.Variable(tf.random.truncated_normal(\n",
        "    [num_hidden, num_labels], stddev=0.1))\n",
        "layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
        "\n",
        "def model_with_dropout(data):\n",
        "  conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
        "  pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
        "  hidden = tf.nn.relu(pool + layer1_biases)\n",
        "  conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
        "  pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
        "  hidden = tf.nn.relu(pool + layer2_biases)\n",
        "  shape = hidden.get_shape().as_list()\n",
        "  reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
        "  hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
        "  drop = tf.nn.dropout(hidden, 0.5)\n",
        "  return tf.matmul(drop, layer4_weights) + layer4_biases"
      ],
      "metadata": {
        "id": "KvdAjf56fkbp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 1000\n",
        "display_step = 100\n",
        "learning_rate = 0.05\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Run training for the given number of steps.\n",
        "for step, (batch_x, batch_y) in enumerate(train_tf_dataset.take(num_steps), 1):  \n",
        "  # Training computation.\n",
        "  with tf.GradientTape() as g:\n",
        "    logits = model_with_dropout(batch_x)\n",
        "    loss = compute_loss(batch_y, logits)\n",
        "    \n",
        "  # Optimizer.\n",
        "  optimizer.minimize(loss, g.watched_variables(), tape=g)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  if step % display_step == 0:\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    train_acc = accuracy(train_prediction, batch_y)\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(model(valid_dataset))\n",
        "    valid_acc = accuracy(valid_prediction, valid_labels)\n",
        "    print(\"step: %i, loss: %f, train acc: %f, validation acc: %f\" % (step, loss, train_acc, valid_acc))\n",
        "\n",
        "test_prediction = tf.nn.softmax(model(test_dataset))\n",
        "test_acc = accuracy(test_prediction, test_labels)\n",
        "print(\"test acc: %f\" % (test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRiqz3AVgy-A",
        "outputId": "59023f29-7c86-43ac-86ae-f3b4cb3d6d0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 100, loss: 1.126998, train acc: 68.750000, validation acc: 56.150000\n",
            "step: 200, loss: 1.110299, train acc: 68.750000, validation acc: 67.970000\n",
            "step: 300, loss: 0.970424, train acc: 56.250000, validation acc: 70.500000\n",
            "step: 400, loss: 0.774720, train acc: 75.000000, validation acc: 73.660000\n",
            "step: 500, loss: 1.406382, train acc: 62.500000, validation acc: 67.330000\n",
            "step: 600, loss: 1.285267, train acc: 62.500000, validation acc: 74.640000\n",
            "step: 700, loss: 0.890334, train acc: 75.000000, validation acc: 77.230000\n",
            "step: 800, loss: 0.651588, train acc: 68.750000, validation acc: 75.060000\n",
            "step: 900, loss: 1.094963, train acc: 62.500000, validation acc: 78.700000\n",
            "step: 1000, loss: 0.645493, train acc: 81.250000, validation acc: 78.220000\n",
            "test acc: 84.990000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muP60Was8Ehf"
      },
      "source": [
        "---\n",
        "Problem 4\n",
        "---------\n",
        "\n",
        "Migrate your best model (with highest accuracy) to graph execution with tf.function instead of running in eager mode. Use tf.config.run_functions_eagerly to test and debug your code.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 1000\n",
        "display_step = 100\n",
        "learning_rate = 0.05\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "graph_model = tf.function(model)\n",
        "\n",
        "# Run training for the given number of steps.\n",
        "for step, (batch_x, batch_y) in enumerate(train_tf_dataset.take(num_steps), 1):  \n",
        "  # Training computation.\n",
        "  with tf.GradientTape() as g:\n",
        "    logits = graph_model(batch_x)\n",
        "    loss = compute_loss(batch_y, logits)\n",
        "    \n",
        "  # Optimizer.\n",
        "  optimizer.minimize(loss, g.watched_variables(), tape=g)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  if step % display_step == 0:\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    train_acc = accuracy(train_prediction, batch_y)\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(model(valid_dataset))\n",
        "    valid_acc = accuracy(valid_prediction, valid_labels)\n",
        "    print(\"step: %i, loss: %f, train acc: %f, validation acc: %f\" % (step, loss, train_acc, valid_acc))\n",
        "\n",
        "test_prediction = tf.nn.softmax(model(test_dataset))\n",
        "test_acc = accuracy(test_prediction, test_labels)\n",
        "print(\"test acc: %f\" % (test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjuVdH7gi0t6",
        "outputId": "a0b2e01d-b39e-49fb-eb66-714478b4cc01"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 100, loss: 0.251008, train acc: 87.500000, validation acc: 84.740000\n",
            "step: 200, loss: 0.478967, train acc: 81.250000, validation acc: 85.260000\n",
            "step: 300, loss: 0.390301, train acc: 81.250000, validation acc: 85.240000\n",
            "step: 400, loss: 0.263747, train acc: 93.750000, validation acc: 85.580000\n",
            "step: 500, loss: 0.782951, train acc: 81.250000, validation acc: 85.560000\n",
            "step: 600, loss: 0.844131, train acc: 68.750000, validation acc: 85.800000\n",
            "step: 700, loss: 0.369759, train acc: 87.500000, validation acc: 85.600000\n",
            "step: 800, loss: 0.579013, train acc: 81.250000, validation acc: 85.040000\n",
            "step: 900, loss: 0.528536, train acc: 81.250000, validation acc: 86.140000\n",
            "step: 1000, loss: 0.104160, train acc: 100.000000, validation acc: 85.760000\n",
            "test acc: 92.160000\n"
          ]
        }
      ]
    }
  ]
}